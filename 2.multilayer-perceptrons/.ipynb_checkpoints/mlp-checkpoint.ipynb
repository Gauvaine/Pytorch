{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77ac0c99",
   "metadata": {},
   "source": [
    "# 多层感知机\n",
    "## 在网络中加入隐藏层\n",
    "\n",
    "我们可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制，\n",
    "使其能处理更普遍的函数关系类型。\n",
    "要做到这一点，最简单的方法是将许多全连接层堆叠在一起。\n",
    "每一层都输出到上面的层，直到生成最后的输出。\n",
    "我们可以把前$L-1$层看作表示，把最后一层看作线性预测器。\n",
    "这种架构通常称为*多层感知机*（multilayer perceptron），通常缩写为*MLP*。\n",
    "下面，我们以图的方式描述了多层感知机（ :numref:`fig_mlp`）。\n",
    "\n",
    "![一个单隐藏层的多层感知机，具有5个隐藏单元](../img/mlp.svg)\n",
    "注意，这两个层都是全连接的。\n",
    "每个输入都会影响隐藏层中的每个神经元，而隐藏层中的每个神经元又会影响输出层中的每个神经元。\n",
    "为了发挥多层架构的潜力，还需要一个额外的关键要素：在仿射变换之后对每个隐藏单元应用非线性的*激活函数*（activation function）$\\sigma$。\n",
    "激活函数的输出（例如，$\\sigma(\\cdot)$）被称为*活性值*（activations）。\n",
    "一般来说，有了激活函数，就不可能再将我们的多层感知机退化成线性模型：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\mathbf{H} & = \\sigma(\\mathbf{X} \\mathbf{W}^{(1)} + \\mathbf{b}^{(1)}), \\\\\n",
    "    \\mathbf{O} & = \\mathbf{H}\\mathbf{W}^{(2)} + \\mathbf{b}^{(2)}.\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "由于$\\mathbf{X}$中的每一行对应于小批量中的一个样本，\n",
    "出于记号习惯的考量，我们定义非线性函数$\\sigma$也以按行的方式作用于其输入，即一次计算一个样本。  \n",
    "但是本节应用于隐藏层的激活函数通常不仅按行操作，也按元素操作。\n",
    "这意味着在计算每一层的线性部分之后，我们可以计算每个活性值，\n",
    "而不需要查看其他隐藏单元所取的值。对于大多数激活函数都是这样。\n",
    "\n",
    "为了构建更通用的多层感知机，我们可以继续堆叠这样的隐藏层：  \n",
    "$\\mathbf{H}^{(1)} = \\sigma_1(\\mathbf{X} \\mathbf{W}^{(1)} + \\mathbf{b}^{(1)})$和$\\mathbf{H}^{(2)} = \\sigma_2(\\mathbf{H}^{(1)} \\mathbf{W}^{(2)} + \\mathbf{b}^{(2)})$，\n",
    "\n",
    "\n",
    "## 激活函数\n",
    "\n",
    "*激活函数*（activation function）通过计算加权和并加上偏置来确定神经元是否应该被激活，它们将输入信号转换为输出的可微运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b126dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c07c58e",
   "metadata": {},
   "source": [
    "### ReLU函数\n",
    "*修正线性单元*（Rectified linear unit，*ReLU*），\n",
    "给定元素$x$，ReLU函数被定义为该元素与$0$的最大值:  \n",
    "\n",
    "**$$\\operatorname{ReLU}(x) = \\max(x, 0).$$**  \n",
    "通俗地说，ReLU函数通过将相应的活性值设为0，仅保留正元素并丢弃所有负元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b584f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)\n",
    "y = torch.relu(x)\n",
    "d2l.plot(x.detach(), y.detach(), 'x', 'relu(x)', figsize=(5, 2.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34647add",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward(torch.ones_like(x), retain_graph=True)\n",
    "d2l.plot(x.detach(), x.grad, 'x', 'grad of relu', figsize=(5, 2.5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
