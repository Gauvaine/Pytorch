{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaa7e72b",
   "metadata": {},
   "source": [
    "# 图像卷积"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8038703d",
   "metadata": {},
   "source": [
    "## 互相关运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47648498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18696ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d(X, K):  #@save\n",
    "    \"\"\"计算二维互相关运算\"\"\"\n",
    "    h, w = K.shape\n",
    "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a60774",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\n",
    "K = torch.tensor([[0.0, 1.0], [2.0, 3.0]])\n",
    "corr2d(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f841e511",
   "metadata": {},
   "source": [
    "## 卷积层\n",
    "卷积层对输入和卷积核权重进行互相关运算，并在添加标量偏置之后产生输出。 所以，卷积层中的两个被训练的参数是卷积核权重和标量偏置。   \n",
    "就像之前随机初始化全连接层一样，在训练基于卷积层的模型时，也随机初始化卷积核权重。  \n",
    "基于上面定义的`corr2d`函数[**实现二维卷积层**]。  \n",
    "在`__init__`构造函数中，将`weight`和`bias`声明为两个模型参数。前向传播函数调用`corr2d`函数并添加偏置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173ca4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.rand(kernel_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return corr2d(x, self.weight) + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2910d68",
   "metadata": {},
   "source": [
    "## 图像中目标的边缘检测\n",
    "如下是[**卷积层的一个简单应用：**]通过找到像素变化的位置，来(**检测图像中不同颜色的边缘**)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee47f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.ones((6, 8))\n",
    "X[:, 2:6] = 0\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e460f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = torch.tensor([[1.0, -1.0]])\n",
    "Y = corr2d(X, K)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d77fb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转置后，之前检测到的垂直边缘消失了。这个[卷积核K只可以检测垂直边缘]，无法检测水平边缘。\n",
    "corr2d(X.t(), K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bd74c2",
   "metadata": {},
   "source": [
    "## 学习卷积核\n",
    "通过仅查看“输入-输出”对，[**学习由`X`生成`Y`的卷积核**]\n",
    "先构造一个卷积层，并将其卷积核初始化为随机张量。接下来，在每次迭代中，比较`Y`与卷积层输出的平方误差，然后计算梯度来更新卷积核。  \n",
    "为了简单起见，在此使用内置的二维卷积层，并忽略偏置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd38d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造一个二维卷积层，它具有1个输入、输出通道和形状为（1，2）的卷积核\n",
    "conv2d = nn.Conv2d(1,1, kernel_size=(1, 2), bias=False)\n",
    "\n",
    "# 这个二维卷积层使用四维输入和输出格式（批量大小、通道、高度、宽度），\n",
    "# 其中批量大小和通道数都为1\n",
    "X = X.reshape((1, 1, 6, 8))\n",
    "Y = Y.reshape((1, 1, 6, 7))# （6-1+1，8-2+1）\n",
    "lr = 3e-2  # 学习率\n",
    "\n",
    "for i in range(10):\n",
    "    Y_hat = conv2d(X)\n",
    "    l = (Y_hat - Y) ** 2\n",
    "    conv2d.zero_grad()\n",
    "    l.sum().backward()\n",
    "    # 迭代卷积核\n",
    "    conv2d.weight.data[:] -= lr * conv2d.weight.grad\n",
    "    if (i + 1) % 2 == 0:\n",
    "        print(f'epoch {i+1}, loss {l.sum():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385ff263",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d.weight.data.reshape((1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550aa63a",
   "metadata": {},
   "source": [
    "## 互相关和卷积\n",
    "回想一下在 :numref:`sec_why-conv`中观察到的互相关和卷积运算之间的对应关系。\n",
    "为了得到正式的*卷积*运算输出，需要执行 :eqref:`eq_2d-conv-discrete`中定义的严格卷积运算，而不是互相关运算。\n",
    "幸运的是，它们差别不大，只需水平和垂直翻转二维卷积核张量，然后对输入张量执行*互相关*运算。\n",
    "\n",
    "值得注意的是，由于卷积核是从数据中学习到的，因此无论这些层执行严格的卷积运算还是互相关运算，卷积层的输出都不会受到影响。\n",
    "为了说明这一点，假设卷积层执行*互相关*运算并学习 :numref:`fig_correlation`中的卷积核，该卷积核在这里由矩阵$\\mathbf{K}$表示。\n",
    "假设其他条件不变，当这个层执行严格的*卷积*时，学习的卷积核$\\mathbf{K}'$在水平和垂直翻转之后将与$\\mathbf{K}$相同。\n",
    "也就是说，当卷积层对 :numref:`fig_correlation`中的输入和$\\mathbf{K}'$执行严格*卷积*运算时，将得到与互相关运算 :numref:`fig_correlation`中相同的输出。\n",
    "\n",
    "为了与深度学习文献中的标准术语保持一致，将继续把“互相关运算”称为卷积运算，尽管严格地说，它们略有不同。\n",
    "此外，对于卷积核张量上的权重，我们称其为*元素*。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
